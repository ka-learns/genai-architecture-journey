{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting Special Tokens\n",
        "\n",
        "Visualize special tokens, attention masks, token IDs."
      ],
      "metadata": {
        "id": "KkLx5rYJz8ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Tokenizer"
      ],
      "metadata": {
        "id": "decNe4eT0H6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Xqks78atzpxF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at special tokens"
      ],
      "metadata": {
        "id": "D-lAYheg0fQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CLS token:\", tokenizer.cls_token)  # Start of sequence\n",
        "print(\"SEP token:\", tokenizer.sep_token)  # End of sequence\n",
        "print(\"PAD token:\", tokenizer.pad_token)  # Fills empty spaces"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjt6pVnq0j_m",
        "outputId": "ed4c2c54-fe39-4c05-c406-345b49eb80e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLS token: [CLS]\n",
            "SEP token: [SEP]\n",
            "PAD token: [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize a Single Sentence"
      ],
      "metadata": {
        "id": "hC9fRPTC2tz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love Hugging Face!\"\n",
        "tokens = tokenizer(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leMVpk4n2vhD",
        "outputId": "d6e31b88-eeb8-4919-e537-8af3b57a3ba2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 1045, 2293, 17662, 2227, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize Sentences"
      ],
      "metadata": {
        "id": "jp_yp3o47ui9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"I love Hugging Face! Its very convenient.\", \"I am learning a lot!\"]\n",
        "tokens = tokenizer(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xabui0D57xnA",
        "outputId": "bf9623c5-c58c-4ad1-ab7f-062e7cde4568"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1045, 2293, 17662, 2227, 999, 2049, 2200, 14057, 1012, 102], [101, 1045, 2572, 4083, 1037, 2843, 999, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding and Truncation"
      ],
      "metadata": {
        "id": "1BekZbhZ9I45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"I love Hugging Face!\", \"AI is amazing!\"]\n",
        "batch = tokenizer(texts, padding=True, truncation=True, max_length=9, return_tensors=\"pt\")\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmJdVR9D9KXg",
        "outputId": "b70d9727-2bbf-4128-f285-ce43d7a5fe10"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  2293, 17662,  2227,   999,   102],\n",
            "        [  101,  9932,  2003,  6429,   999,   102,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Saw multiple sentences, with special tokens (CLS, SEP, PAD)\n",
        "* Observed padding which will make the input sequences of the same length by adding PAD special tokens with a token id of 0 (ignored by the model)\n",
        "* Observed truncation which removes extra tokens that are greater than max length\n",
        "* Observed max_length which is related to truncation process as well as the padding process\n",
        "* Returned the data in a tensor Object, which will be commonly used across models when handling batches of data, stores multi-dimensional array as objects\n",
        "* Data can be returned in pyTorch objects, TensorFlow objects, numpy arrays, and python lists\n"
      ],
      "metadata": {
        "id": "cLWj9_TLBpYr"
      }
    }
  ]
}